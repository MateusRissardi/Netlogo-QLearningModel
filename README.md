# Cliff Walking Model com Q-Learning no NetLogo

Este reposit√≥rio cont√©m uma implementa√ß√£o do cl√°ssico problema de *Cliff Walking* (Caminhada no Penhasco) utilizando o algoritmo de Aprendizado por Refor√ßo **Q-Learning**, desenvolvido na plataforma **NetLogo**.

## üéØ Objetivo

O principal objetivo deste projeto √© demonstrar a capacidade de integra√ß√£o de algoritmos de **Aprendizado por Refor√ßo (Reinforcement Learning)** dentro do NetLogo. Ele serve como uma prova de conceito de que √© poss√≠vel sofisticar o comportamento e a tomada de decis√£o dos agentes (turtles) para al√©m de regras simples baseadas em "se/ent√£o", permitindo que eles aprendam com o ambiente atrav√©s de tentativa e erro.

## ‚öôÔ∏è Como Funciona

O modelo simula um ambiente de grade (grid world) onde um agente deve navegar de um ponto inicial at√© um objetivo, evitando cair em um "penhasco".

### O Ambiente
* **√Årea Segura (Branco):** O agente pode transitar livremente. Cada passo custa **-1** (recompensa negativa) para incentivar o caminho mais curto.
* **Penhasco (Vermelho):** Se o agente pisar aqui, ele recebe uma penalidade severa de **-10** e √© enviado de volta ao in√≠cio.
* **Objetivo (Verde):** Ao alcan√ßar este ponto, o agente recebe uma recompensa de **+10**.

### O Algoritmo (Q-Learning)
O agente n√£o conhece as regras do mapa inicialmente. Ele utiliza uma **Q-Table** para armazenar o valor esperado (utilidade) de realizar cada a√ß√£o (Cima, Baixo, Esquerda, Direita) em cada estado (patch).

A atualiza√ß√£o do conhecimento acontece a cada passo seguindo a equa√ß√£o de Bellman:

$$Q(s,a) \leftarrow Q(s,a) + \alpha [R + \gamma \max Q(s',a') - Q(s,a)]$$

Onde:
* **$\alpha$ (alpha):** Taxa de aprendizado (`learnrate`).
* **$\gamma$ (gamma):** Fator de desconto (`disctax`).
* **$R$:** Recompensa imediata (`reward`).

### Estrat√©gia de Decis√£o ($\epsilon$-Greedy)
O agente utiliza uma estrat√©gia **Epsilon-Greedy** para balancear entre:
1.  **Explora√ß√£o (Exploration):** Tentar uma a√ß√£o aleat√≥ria para descobrir novos caminhos (controlado pela vari√°vel `exprob`).
2.  **Aproveitamento (Exploitation):** Escolher a melhor a√ß√£o conhecida at√© o momento baseada na Q-Table.

## üöÄ Como Usar

### Pr√©-requisitos
* [NetLogo 6.x](https://ccl.northwestern.edu/netlogo/) instalado.

### Executando o Modelo
1.  Baixe o arquivo `CliffWalkingModel-QLearning.nlogo` deste reposit√≥rio.
2.  Abra o arquivo no NetLogo.
3.  Clique no bot√£o **Setup** para inicializar o ambiente e as vari√°veis.
4.  Clique no bot√£o **Go** para iniciar a simula√ß√£o.

### Par√¢metros (Vari√°veis Globais)
No c√≥digo (`setup-valores`), voc√™ pode ajustar os hiperpar√¢metros do aprendizado:
* `exprob`: Probabilidade de explora√ß√£o (inicialmente 25%).
* `learnrate`: Taxa de aprendizado (inicialmente 0.2).
* `disctax`: Fator de desconto para recompensas futuras (inicialmente 0.1).

## üìä O Que Observar

Ao rodar o modelo, note que:
1.  No in√≠cio, o agente ("turtle" vermelha) se move erraticamente, caindo frequentemente na zona vermelha.
2.  Com o passar do tempo (ticks), a `totalreward` tende a aumentar.
3.  Eventualmente, o agente aprende o caminho "√≥timo" ou "seguro" at√© a zona verde, demonstrando a converg√™ncia do algoritmo Q-Learning.

## üõ† Tecnologias

* **Linguagem:** NetLogo
* **Conceitos:** Intelig√™ncia Artificial, Aprendizado por Refor√ßo, Q-Learning.

## üìù Licen√ßa

Este projeto √© de c√≥digo aberto. Sinta-se √† vontade para contribuir ou modificar para fins educacionais.
